{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b8591f",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "#### <a href=\"https://lioritan.github.io\">Lior Friedman</a>\n",
    "\n",
    "## Tutorial 09 - Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5b1d",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Self-Supervised Learning - a short overview](#Self-Supervised-Learning---a-short-overview)\n",
    "* [Contrastive Learning](#-fff)\n",
    "* [Contrastive Loss Functions](#-fff)\n",
    "* [SimCLR](#-d)\n",
    "* [MoCo]\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3326941",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/50/000000/idea.png\" style=\"height:50px;display:inline\"> Self-Supervised Learning - a short overview\n",
    "---\n",
    "* Picking the right representation matters!\n",
    "    * <img src=\"./assets/selfsup_representation.png\" style=\"height:300px\">\n",
    "* <b>Deep unsupervised learning</b> - Learn representation without labels (e.g. Autoencoders).\n",
    "* <b>Self-supervised learning</b> - Create your own supervision using pretext tasks.\n",
    "    * Some examples: <img src=\"./assets/selfsup_pretexts.png\" style=\"height:250px\">\n",
    "* <b>Idea</b>: withhold some part of the data and then task a neural network to predict it from the remaining parts.\n",
    "* Details decide what proxy loss or pretext task the network tries to solve, and depending on the quality of the task, good semantic features can be obtained without actual labels.\n",
    "* Can take advantage of vast amounts of unlabeled data (e.g. the internet), and unlabeled data is usually cheap (in time and labor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21757f0f",
   "metadata": {},
   "source": [
    "<img src=\"./assets/selfsup_pretexts2.png\" style=\"height:200px;inline\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ead56",
   "metadata": {},
   "source": [
    "#### Four main families of algorithms:\n",
    "1. Deep Metric Learning - learn a metric that encourages similarity between transformed versions of the input, uses <b> contrastive loss</b>.\n",
    "2. Self-Distillation - feed two different views of an input to two encoders, try to predict one encoding from the second.\n",
    "3. Deep Canonical Correlation - based on CCA (Canonical Correlation Analysis), find transformations that maximize correlation of paired input views.\n",
    "4. Masked Modeling - originated in images, undo a degradation such as de-colorization, noise, shuffling image patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6c052",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/magnet.png\" style=\"height:50px;display:inline\"> Contrastive Learning\n",
    "---\n",
    "* Methods that rely on a contrastive loss: <b>similar things should be close, giving low loss, and dissimilar things should be far</b>.\n",
    "* Basically the same as a classification problem between similar and dissimilar things.\n",
    "* Given example $x$, learn an embedding/encoding such that\n",
    "    * $$\\mathrm{score}(f(x),f(x^+))\\gg\\mathrm{score}(f(x),f(x^-))$$\n",
    "    * $x^+$ is a data point similar to $x$, referred to as a positive sample.\n",
    "    * $x^-$ is a data point dissimilar to $x$, referred to as a negative sample.\n",
    "* <img src=\"./assets/selfsup_contrast_augs.png\" style=\"height:200px;\">\n",
    "* Positive samples are obtained using transformations and augmentations of the original data point.\n",
    "* Negative samples can be chosen as other data points, but picking challeging $x^-$ can significantly improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cf554",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/bad-decision.png\" style=\"height:50px;display:inline\"> Contrastive loss functions\n",
    "---\n",
    "* <b>Triplet loss</b>: $$\\mathcal{L}_{triplet}(x,x^+,x^-)=\\max(0, ||f(x)-f(x^+)||^2_2-||f(x)-f(x^-)||^2_2+\\epsilon)$$\n",
    "    * One-to-one ratio of positive and negative examples.\n",
    "    * $\\epsilon$ is a hyperparameter for the minimal margin between similar and dissimilar things.\n",
    "* <b>Multi-Class N-pair loss</b> (commonly refered to as the InfoNCE loss): $$\\mathcal{L}_{N-pair}(x,x^+,\\{x^-_i\\}_{i=1}^N)=-\\log\\frac{\\exp(f(x)^Tf(x^+))}{\\exp(f(x)^Tf(x^+))+\\sum_{i=1}^N\\exp(f(x)^Tf(x^-_i))}$$\n",
    "    * Can have multiple negative samples per positive sample.\n",
    "    * If $N=1$, this is multi-class softmax.\n",
    "    * <img src=\"./assets/selfsup_infonce_loss.png\" style=\"height:100px;\">\n",
    "    * Usually add a temperature parameter $\\tau$ and divide all inner products by it (e.g. $\\exp(f(x)^Tf(x^+)/\\tau)$)\n",
    "* The original <b> InfoNCE </b> (Noise Contrastive Estimation): Same idea as Multi-Class N-pair loss, but we assume that we are also given a context vector $c$, and would like to maximize the probability of classifing $x$ correctly.\n",
    "    * $$\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\mathbb{E}_x\\left [ \\log \\frac{f(x,c)}{\\sum_{x'\\in X}f(x',c)} \\right ]$$\n",
    "    * Where $f(x, c)\\propto\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$ estimates the true density ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500fd82f",
   "metadata": {},
   "source": [
    "#### Key ingredients to contrastive learning:\n",
    "1. Heavy Data Augmentation: Needed to create noise versions of a sample to feed into the loss as positive samples. It introduces the <b>non-essential variations</b> into examples without modifying semantic meanings and thus encourages the model to learn the essential part of the representation.\n",
    "\n",
    "2. Large Batch Size: Needed in order for the loss function to cover a <b>diverse enough collection of negative samples</b>, challenging enough for the model to learn meaningful representation to distinguish different examples.\n",
    "\n",
    "3. Hard Negative Mining: Hard negative samples should have different labels from the anchor sample, but have embedding features very close to the anchor embedding. This is easier if we do have ground truth labels. However, it becomes tricky to do hard negative mining when we want to remain unsupervised. <b>It is also important to avoid false negatives</b>.\n",
    "<img src=\"./assets/selfsup_contrast_fn.png\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf860ec",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:50px;display:inline\"> Exercise - InfoNCE as ELBO\n",
    "---\n",
    "The mutual information (MI) of two random variables $X, Y$ is a measure of the mutual dependence between the two variables.\n",
    "More specifically, it quantifies the \"amount of information\" obtained about one random variable by observing the other random variable.\n",
    "<img src=\"./assets/selfsup_mi.png\" style=\"height:200px;\">\n",
    "$$I(X;Y)=D_{\\mathrm{KL}}(P_{(X,Y)}||P_{X}\\otimes P_{Y})$$ where $\\otimes$ is an outer product.\n",
    "\n",
    "In InfoNCE, the score function $f(x,c)$ is an estimate of the true density ratio $\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$ (this is the optimal $f$, but is expensive to compute).\n",
    "\n",
    "Show that if the batch is sufficiently representative, the InfoNCE loss is a lower bound on the MI of $x$ and $c$, $I(x;c)\\geq -\\mathcal{L}_{\\mathrm{InfoNCE}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cd7f5",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "---\n",
    "$$\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\mathbb{E}_x\\left [ \\log \\frac{f(x,c)}{\\sum_{x'\\in X}f(x',c)} \\right ]$$\n",
    "For the optimal choice, we have $f(x, c)=\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$.\n",
    "$$\\mathcal{L}_{\\mathrm{InfoNCE}}^{opt}=-\\mathbb{E}_x\\left [ \\log \\frac{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}}{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}+\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')}} \\right ]$$\n",
    "$$=\\mathbb{E}_x\\left [ \\log \\frac{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}+\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')}}{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}} \\right ]=\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')} \\right ]$$\n",
    "\n",
    "Assuming the batch is sufficiently representative, the mean of negative examples is representative of the expectation\n",
    "$$\\approx\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}(N-1)\\mathbb{E}_{x'\\sim \\mathcal{X}_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')} \\right ]$$\n",
    "$$=\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}(N-1) \\right ]\\geq \\mathbb{E}_x\\log\\left [\\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}N \\right ]$$\n",
    "$$=\\log N+\\mathbb{E}_x\\log\\left [\\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)} \\right ]=\\log N-I(x;c)$$\n",
    "\n",
    "Putting it all together, we have\n",
    "$$I(x;c)\\geq \\log N-\\mathcal{L}_{\\mathrm{InfoNCE}}^{opt}$$, giving us a lower bound.\n",
    "\n",
    "We note that as $N$ increases, the approximation of the expectation becomes more accurate, and the rhs increases, meaning we would prefer to use a high value of $N$ to get a tighter bound.\n",
    "We can also use this to show that for the N-pair loss, $$I(f(x);f(x^+))\\geq \\log N-\\mathcal{L}_{N-pairs}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6387433",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/nolan/64/collapse-arrow.png\" style=\"height:50px;display:inline\"> SimCLR\n",
    "---\n",
    "* Variant of the InfoNCE loss, <b> NT-Xent </b> (Normalized Temperature-scaled cross entropy loss): Apply a neural network encoder $f(\\cdot)$ that extracts <b>representation vectors</b> from augmented data examples (e.g. ResNet). Apply a small neural network head $g(\\cdot)$ to map to latent space for contrastive loss. \n",
    "    * $z_i=g(f(x_i))$.\n",
    "    * Sample $N$ data points, run \n",
    "    * $$\\ell_{i,j} = -\\log{\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i, z_k)/\\tau)}}$$ where $\\text{sim}(z_i, z_j) = \\frac{z_i^Tz_j}{\\left\\Vert z_i \\right\\Vert \\left\\Vert z_j \\right\\Vert}$ (cosine similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ef71d",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* <a href=\"https://sites.google.com/view/berkeley-cs294-158-sp20/home\"> CS294-158-SP20-Deep Unsupervised Learning </a> @ UC Berkeley\n",
    "* <a href=\"http://cs231n.stanford.edu/2021/index.html\"> CS231n-Convolutional Neural Networks for Visual Recognition </a> @ Stanford\n",
    "* <a href=\"https://lilianweng.github.io/posts/2021-05-31-contrastive/\"> Weng, Lilian. (May 2021). Contrastive representation learning. Lil’Log </a>\n",
    "* A Cookbook of Self-Supervised Learning, Balestriero et al. 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c42c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
