{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b8591f",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "#### <a href=\"https://lioritan.github.io\">Lior Friedman</a>\n",
    "\n",
    "## Tutorial 09 - Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5b1d",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Self-Supervised Learning](#-Self-Supervised-Learning)\n",
    "* [Contrastive Learning](#-Contrastive-Learning)\n",
    "* [Contrastive Loss Functions](#-Contrastive-loss-functions)\n",
    "* [SimCLR](#-SimCLR)\n",
    "* [MoCo](#-MoCo)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3326941",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/50/000000/idea.png\" style=\"height:50px;display:inline\"> Self-Supervised Learning\n",
    "---\n",
    "* Picking the right representation matters!\n",
    "    * <img src=\"./assets/selfsup_representation.png\" style=\"height:300px\">\n",
    "* <b>Deep unsupervised learning</b> - Learn representation without labels (e.g. Autoencoders).\n",
    "* <b>Self-supervised learning</b> - Create your own supervision using pretext tasks.\n",
    "    * Some examples: <img src=\"./assets/selfsup_pretexts.png\" style=\"height:250px\">\n",
    "* <b>Idea</b>: withhold some part of the data and then task a neural network to predict it from the remaining parts.\n",
    "* Details decide what proxy loss or pretext task the network tries to solve, and depending on the quality of the task, good semantic features can be obtained without actual labels.\n",
    "* Can take advantage of vast amounts of unlabeled data (e.g. the internet), and unlabeled data is usually cheap (in time and labor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c28d0",
   "metadata": {},
   "source": [
    "<img src=\"./assets/selfsup_pretexts2.png\" style=\"height:200px;inline\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84788446",
   "metadata": {},
   "source": [
    "#### Four main families of algorithms:\n",
    "1. Deep Metric Learning - learn a metric that encourages similarity between transformed versions of the input, uses <b> contrastive loss</b>.\n",
    "2. Self-Distillation - feed two different views of an input to two encoders, try to predict one encoding from the second.\n",
    "3. Deep Canonical Correlation - based on CCA (Canonical Correlation Analysis), find transformations that maximize correlation of paired input views.\n",
    "4. Masked Modeling - originated in images, undo a degradation such as de-colorization, noise, shuffling image patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24218f5c",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/magnet.png\" style=\"height:50px;display:inline\"> Contrastive Learning\n",
    "---\n",
    "* Methods that rely on a contrastive loss: <b>similar things should be close, giving low loss, and dissimilar things should be far</b>.\n",
    "* Basically the same as a classification problem between similar and dissimilar things.\n",
    "* Given example $x$, learn an embedding/encoding such that\n",
    "    * $$\\mathrm{score}(f(x),f(x^+))\\gg\\mathrm{score}(f(x),f(x^-))$$\n",
    "    * $x^+$ is a data point similar to $x$, referred to as a positive sample.\n",
    "    * $x^-$ is a data point dissimilar to $x$, referred to as a negative sample.\n",
    "* <img src=\"./assets/selfsup_contrast_augs.png\" style=\"height:200px;\">\n",
    "* Positive samples are obtained using transformations and augmentations of the original data point.\n",
    "* Negative samples can be chosen as other data points, but picking challeging $x^-$ can significantly improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17165aaa",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/bad-decision.png\" style=\"height:50px;display:inline\"> Contrastive loss functions\n",
    "---\n",
    "* <b>Triplet loss</b>: $$\\mathcal{L}_{triplet}(x,x^+,x^-)=\\max(0, ||f(x)-f(x^+)||^2_2-||f(x)-f(x^-)||^2_2+\\epsilon)$$\n",
    "    * One-to-one ratio of positive and negative examples.\n",
    "    * $\\epsilon$ is a hyperparameter for the minimal margin between similar and dissimilar things.\n",
    "* <b>Multi-Class N-pair loss</b> (commonly refered to as the InfoNCE loss): $$\\mathcal{L}_{N-pair}(x,x^+,\\{x^-_i\\}_{i=1}^N)=-\\log\\frac{\\exp(f(x)^Tf(x^+))}{\\exp(f(x)^Tf(x^+))+\\sum_{i=1}^N\\exp(f(x)^Tf(x^-_i))}$$\n",
    "    * Can have multiple negative samples per positive sample.\n",
    "    * If $N=1$, this is multi-class softmax.\n",
    "    * <img src=\"./assets/selfsup_infonce_loss.png\" style=\"height:100px;\">\n",
    "    * Usually add a temperature parameter $\\tau$ and divide all inner products by it (e.g. $\\exp(f(x)^Tf(x^+)/\\tau)$)\n",
    "* The original <b> InfoNCE </b> (Noise Contrastive Estimation): Same idea as Multi-Class N-pair loss, but we assume that we are also given a context vector $c$, and would like to maximize the probability of classifing $x$ correctly.\n",
    "    * $$\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\mathbb{E}_x\\left [ \\log \\frac{f(x,c)}{\\sum_{x'\\in X}f(x',c)} \\right ]$$\n",
    "    * Where $f(x, c)\\propto\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$ estimates the true density ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634e337",
   "metadata": {},
   "source": [
    "#### Key ingredients to contrastive learning:\n",
    "1. Heavy Data Augmentation: Needed to create noise versions of a sample to feed into the loss as positive samples. It introduces the <b>non-essential variations</b> into examples without modifying semantic meanings and thus encourages the model to learn the essential part of the representation.\n",
    "\n",
    "2. Large Batch Size: Needed in order for the loss function to cover a <b>diverse enough collection of negative samples</b>, challenging enough for the model to learn meaningful representation to distinguish different examples.\n",
    "\n",
    "3. Hard Negative Mining: Hard negative samples should have different labels from the anchor sample, but have embedding features very close to the anchor embedding. This is easier if we do have ground truth labels. However, it becomes tricky to do hard negative mining when we want to remain unsupervised. <b>It is also important to avoid false negatives</b>.\n",
    "<img src=\"./assets/selfsup_contrast_fn.png\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617a149",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:50px;display:inline\"> Exercise - InfoNCE as ELBO\n",
    "---\n",
    "The mutual information (MI) of two random variables $X, Y$ is a measure of the mutual dependence between the two variables.\n",
    "More specifically, it quantifies the \"amount of information\" obtained about one random variable by observing the other random variable.\n",
    "<img src=\"./assets/selfsup_mi.png\" style=\"height:200px;\">\n",
    "$$I(X;Y)=D_{\\mathrm{KL}}(P_{(X,Y)}||P_{X}\\otimes P_{Y})$$ where $\\otimes$ is an outer product.\n",
    "\n",
    "In InfoNCE, the score function $f(x,c)$ is an estimate of the true density ratio $\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$ (this is the optimal $f$, but is expensive to compute).\n",
    "\n",
    "Show that if the batch is sufficiently representative, the InfoNCE loss is a lower bound on the MI of $x$ and $c$, $I(x;c)\\geq -\\mathcal{L}_{\\mathrm{InfoNCE}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e11f3e",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution\n",
    "---\n",
    "$$\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\mathbb{E}_x\\left [ \\log \\frac{f(x,c)}{\\sum_{x'\\in X}f(x',c)} \\right ]$$\n",
    "For the optimal choice, we have $f(x, c)=\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}$.\n",
    "$$\\mathcal{L}_{\\mathrm{InfoNCE}}^{opt}=-\\mathbb{E}_x\\left [ \\log \\frac{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}}{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}+\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')}} \\right ]$$\n",
    "$$=\\mathbb{E}_x\\left [ \\log \\frac{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}+\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')}}{\\frac{\\mathrm{P}(x|c)}{\\mathrm{P}(x)}} \\right ]=\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}\\sum_{x'\\in X_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')} \\right ]$$\n",
    "\n",
    "Assuming the batch is sufficiently representative, the mean of negative examples is representative of the expectation\n",
    "$$\\approx\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}(N-1)\\mathbb{E}_{x'\\sim \\mathcal{X}_{neg}}\\frac{\\mathrm{P}(x'|c)}{\\mathrm{P}(x')} \\right ]$$\n",
    "$$=\\mathbb{E}_x\\log\\left [1+ \\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}(N-1) \\right ]\\geq \\mathbb{E}_x\\log\\left [\\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)}N \\right ]$$\n",
    "$$=\\log N+\\mathbb{E}_x\\log\\left [\\frac{\\mathrm{P}(x)}{\\mathrm{P}(x|c)} \\right ]=\\log N-I(x;c)$$\n",
    "\n",
    "Putting it all together, we have\n",
    "$$I(x;c)\\geq \\log N-\\mathcal{L}_{\\mathrm{InfoNCE}}^{opt}$$, giving us a lower bound.\n",
    "\n",
    "We note that as $N$ increases, the approximation of the expectation becomes more accurate, and the rhs increases, meaning we would prefer to use a high value of $N$ to get a tighter bound.\n",
    "We can also use this to show that for the N-pair loss, $$I(f(x);f(x^+))\\geq \\log N-\\mathcal{L}_{N-pairs}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d12d1",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/nolan/64/collapse-arrow.png\" style=\"height:50px;display:inline\"> SimCLR\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/2002.05709\"> Simple Framework for Contrastive Learning of Visual Representations (SimCLR)</a> learns visual representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss.\n",
    "<img src=\"./assets/selfsup_simclr.png\" style=\"height:300px;\">\n",
    "\n",
    "1. For a batch of $N$ samples, randomly apply two data augmentations on each -> $2N$ total samples $$\\{(\\tilde{x_i},\\tilde{x_i'})\\}_{i=1}^N=\\{(t(x_i),t'(x_i))\\}_{i=1}^N, \\quad t,t'\\sim \\tau$$\n",
    "    * For each positive pair, we have $2(N-1)$ negative samples.\n",
    "2. Apply a neural network encoder $f(\\cdot)$ that extracts <b>representation vectors</b> $h_j$ from augmented data examples (e.g. ResNet). \n",
    "    * <b>The representation layer $h$ is used for downstream tasks</b>.\n",
    "3. Apply a small neural network head $g(\\cdot)$ to map to latent space $z$ for contrastive loss.\n",
    "    * $z_i=g(h_i)=g(f(\\tilde{x_i}))$.\n",
    "    * The latent space is only used for the contrastive loss optimization.\n",
    "    * <b>This nonlinear projection really helps</b>. Why? Possibly since using $g$ can store transformations specific to the contrastive task, letting the representation $h$ hold more general information.\n",
    "4. $$\\ell^{i,j}_{\\mathrm{SimCLR}}=-\\log{\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i, z_k)/\\tau)}}$$ .\n",
    "    * $\\text{sim}(z_i, z_j) = \\frac{z_i^Tz_j}{\\left\\Vert z_i \\right\\Vert \\left\\Vert z_j \\right\\Vert}$ (cosine similarity).\n",
    "    * This is called <b> NT-Xent</b> (Normalized Temperature-scaled cross entropy) loss.\n",
    "    * Average over all positive pairs.\n",
    "<img src=\"./assets/selfsup_simclr.gif\" style=\"height:300px;\"> <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\"> Source</a>\n",
    "\n",
    "Notes:\n",
    "* <b>SimCLR needs a large batch size (2048 seems to be enough) to incorporate enough negative samples to achieve good performance.</b>\n",
    "* The combinations of image transformations used to generate corresponding views are critical.\n",
    "<img src=\"./assets/selfsup_simclr_pretexts.png\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba68b9",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/gyroscope.png\" style=\"height:50px;display:inline\"> MoCo\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/1911.05722\"> Momentum Contrast (MoCo)</a> provides a framework of unsupervised learning visual representation as a dynamic dictionary look-up. \n",
    "* The <b>\"keys\" (tokens)</b> in the dictionary are sampled from data (e.g., images or patches) and are represented by an encoder network.\n",
    "* Unsupervised learning trains encoders (by minimizing a contrastive loss) to perform <b>dictionary look-up: an encoded “query”</b> should be similar to its matching key and dissimilar to others.\n",
    "* The dictionary is a large FIFO (First In First Out) queue of encoded samples.\n",
    "* <b>This queue decouples the dictionary size from the mini-batch size</b>, allowing it to be large, so we can have many examples to compare to.\n",
    "<img src=\"./assets/selfsup_moco.png\" style=\"height:300px;\">\n",
    "1. Given a query $x_q$, apply encoder $q=f_q(x_q)$. Assume we have a single positive key in the dictionary $k^+$ that matches $q$, and the other $N-1$ keys are negative.\n",
    "    * Keys are encoded via a momentum encoder (keeps a momentum-based moving average of the query encoder parameters).\n",
    "    * In training, we push an encoded representation of a different augmentation of $x$ as the positive key $k^+$.\n",
    "2. $$\\mathcal{L}_{\\mathrm{MoCo}}=-\\log \\frac{\\exp(q\\cdot k^+/\\tau)}{\\sum_{i=1}^N\\exp(q\\cdot k_i/\\tau)}$$\n",
    "<img src=\"./assets/selfsup_moco_nograd.png\" style=\"height:300px;\">\n",
    "3. The MoCo dictionary is not differentiable as a queue, so we cannot rely on back-propagation to update the key encoder $f_k$, which is why we use a momentum update $\\theta_k=m\\theta_k+(1-m)\\theta_q$.\n",
    "\n",
    "Notes:\n",
    "* MoCo V2 adds two ideas from SimCLR: (1) an MLP projection head and (2) stronger data augmentation.\n",
    "* Overall, needs much smaller batch size and memory footprint compared to SimCLR with the same number of parameters.\n",
    "* But a large SIMCLR model is still better\n",
    "<img src=\"./assets/selfsup_compare.png\" style=\"height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dc22c",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* General Self-Supervised Learning - <a href=\"https://www.youtube.com/watch?v=dMUes74-nYY\">Lecture 7 Self-Supervised Learning -- UC Berkeley Spring 2020 - CS294-158 Deep Unsupervised Learning</a>,\n",
    "* SimCLR - <a href=\"https://www.youtube.com/watch?v=APki8LmdJwY\">SimCLR Explained!</a>,\n",
    "* MoCo - <a href=\"https://www.youtube.com/watch?v=LvHwBQF14zs\">Momentum Contrastive Learning</a>,\n",
    "* InfoNCE bound and CPC - <a href=\"https://www.youtube.com/watch?v=zNKMHj1eLa0\">Contrastive Predictive Coding (CPC)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9389a0",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* <a href=\"https://github.com/taldatech/ee046211-deep-learning/blob/main/ee046211_tutorial_09_self_supervised_representation_learning.ipynb\"> ee045211 - Deep Learning </a> @ Technion\n",
    "* <a href=\"https://sites.google.com/view/berkeley-cs294-158-sp20/home\"> CS294-158-SP20-Deep Unsupervised Learning </a> @ UC Berkeley\n",
    "* <a href=\"http://cs231n.stanford.edu/2021/index.html\"> CS231n-Convolutional Neural Networks for Visual Recognition </a> @ Stanford\n",
    "* <a href=\"https://lilianweng.github.io/posts/2021-05-31-contrastive/\"> Weng, Lilian. (May 2021). Contrastive representation learning. Lil’Log </a>\n",
    "* A Cookbook of Self-Supervised Learning, Balestriero et al. 2023\n",
    "* <a href=\"https://paperswithcode.com/method/contrastive-predictive-coding\">Contrastive Predictive Coding</a>\n",
    "* <a href=\"https://paperswithcode.com/method/simclr\"> Simple Framework for Contrastive Learning of Visual Representations (SimCLR)</a>\n",
    "* <a href=\"https://paperswithcode.com/method/moco\">Momentum Contrast</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
