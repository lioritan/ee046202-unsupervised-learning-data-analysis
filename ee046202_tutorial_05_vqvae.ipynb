{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzMW3sFFObBP"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "#### <a href=\"https://www.linkedin.com/in/dan-haramati/\">Dan Haramati</a>\n",
    "\n",
    "## Tutorial 05 - Deep Unsupervised Learning - Vector Quantized VAE (VQ-VAE)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn519pEnObBU"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [VAE Reminder](#VAE-Reminder)\n",
    "* [Motivation and Introduction](#Motivation-&-Introduction)\n",
    "* [Model](#Model)\n",
    "  * [Components and Forward Data Flow](#Components-and-Forward-Data-Flow)\n",
    "  * [Discrete Latent Variables](#Discrete-Latent-Variables)\n",
    "* [Training](#Training)\n",
    "  * [Propagating Gradients Through the Dictionary](#Propagating-Gradients-Through-the-Dictionary)\n",
    "* [Generation](#Generation)\n",
    "  * [Learning the Prior with Autoregressive Generative Models](#Learning-the-Prior-with-Autoregressive-Generative-Models)\n",
    "* [Short Summary](#Short-Summary)\n",
    "* [VQ-VAE 2](#VQ-VAE-2)\n",
    "  * [Updating the Dictionary with an EMA](#Updating-the-Dictionary-with-an-EMA)\n",
    "  * [Hierarchical Encoder Architecture](#Hierarchical-Encoder-Architecture)\n",
    "  * [Hierarchical Generation](#Hierarchical-Generation)\n",
    "* [Applications](#Applications)\n",
    "* [Credits](#Credits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpmJyFTwQ0jZ"
   },
   "source": [
    "###  <img src=\"https://img.icons8.com/bubbles/100/000000/qr-code.png\" style=\"height:20px;display:inline\"> Variational Autoencoder (VAE) - Reminder\n",
    "---\n",
    "The VAE is a deep generative model consisting of a probabalistic encoder and decoder.\n",
    "\n",
    "Notation:\n",
    "1. $X$ - the data we want to model (e.g. images of dogs)\n",
    "2. $z$ - the latent variable (this is the *imagination*, the hidden variable that describes the data, we have seen this before)\n",
    "3. $p_{\\theta}(X)$ - the parameterized probability distribution of the data (e.g. the distribution of all dogs' images in the world). Also, the **evidence**.\n",
    "4. $p(z)$ - the probability distribution of the latent variables (the source of the imagination, the brain in this case or the distribution of dogs' images feaures/hidden representations). The **prior**.\n",
    "5. $p_{\\theta}(X|z)$ - the parameterized distribution of data generation **given latent variable** (given the features we want the dog to have, the probability of images that satisfy these conditions, turning imagination to real image). The **likelihood** (remember MLE?).\n",
    "6. $p_{\\theta}(z|X)$ - the parameterized distribution of latent variables **given data** (given the image of dog, the probability of latent features that satisfy this image). The **posterior**.\n",
    "\n",
    "The VAE objective is to maximize the ELBO of the data:\n",
    "$$ \\mathcal{L}_{VAE} = -\\mathbb{E}_{q_{\\phi}(z|X)}[\\log p_{\\theta}(X|z)] + D_{KL}[q_{\\phi}(z|X)|| p(z)]  $$\n",
    "\n",
    "Where ${q_{\\phi}(z|X)}$ is the Gaussian approximation of the posterior.\n",
    "The reconstruction loss (first term) is in charge of training the encoder to produce informative latent representations and the decoder to reconstruct the image from them while the KL loss (second term) acts as a regularization term for the encoded latent space. Minimizing the KL ensures the posterior will not be too far from the prior which should enable generating new data by sampling from the simple prior and decoding using the trained decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKLEoWXkRcWl"
   },
   "source": [
    "<img src=\"assets/vae_lilian_weng.png\" height=\"500\">\n",
    "\n",
    "* <a href=\"https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#beta-vae\">Image by Lilian Weng</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GJkRBPbKImW"
   },
   "source": [
    "The probabilistic encoding induces a **continuous** latent space which allows smooth latent interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img src=\"assets/mnist_interpolation.png\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVUA23hkKJ8S"
   },
   "source": [
    "<img src=\"assets/celeb_interpolation.png?raw=1\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uP9SeAmObBU"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/50/000000/party-baloons.png\" style=\"height:50px;display:inline\"> Motivation & Introduction\n",
    "---\n",
    "The VQ-VAE was presented in a series of papers by DeepMind researchers:\n",
    "- [\"*Neural Discrete Representation Learning*\", Van Den Oord et al. (NeurIPS 2017)](https://arxiv.org/abs/1711.00937)\n",
    "- [\"*Generating Diverse High-Fidelity Images with VQ-VAE-2*\", Razavi, Van Den Oord et al. (NeurIPS 2019)](https://arxiv.org/abs/1906.00446)\n",
    "\n",
    "The main idea is to combine the VAE framework with **discrete** latent representations. \\\n",
    "The motivation for the discrete latent space comes from the modalities which the VQ-VAE aims to model:\n",
    "- **Language** is discrete in nature.\n",
    "- **Images** can be described concisely by language.\n",
    "\n",
    "Therefore, the VQ-VAE latent space aims to capture the essence of the data via a compact discrete representation.\n",
    "\n",
    "\\\n",
    "The steps for learning a VQ-VAE based generative model are as follows:\n",
    "1. Learn a discrete latent representation with the VQ-VAE architecture.\n",
    "2. Learn a prior over the discrete latent space with a generative model of your choice.\n",
    "3. Sample from the learned prior model and decode using the VQ-VAE decoder to generate new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P9KJUs4XEv4"
   },
   "source": [
    "### Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JM9orJyX9H7"
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/module\" style=\"height:50px;display:inline\"> **Components and Forward Data Flow**\n",
    "\n",
    "The VQ-VAE model is composed of an encoder $E_{\\phi}$, decoder $D_{\\theta}$ and a discrete latent embedding space $e=\\{e_i\\}_{i=1}^{K}$. The parameters of the model are the paramaters of the encoder and decoder neural networks, $\\phi$ and $\\theta$ respectively, as well as the set of dicrete latent values $e$.\n",
    "\n",
    "The data is encoded to the latent space with the encoder. The encoded values are then converted to values from the discrete embedding space. This discretized encoding is then passed to the decoder to reconstruct the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_bYHMBOiIb4"
   },
   "source": [
    "<img src=\"assets/vqvae_discrete.png?raw=1\" style=\"height:300px\">\n",
    "\n",
    "[Image source](https://dl.acm.org/doi/fullHtml/10.1145/3472538.3472584)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCXnVkOTObBZ"
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/grid\" style=\"height:50px;display:inline\"> **Discrete Latent Variables**\n",
    "\n",
    "Define a latent embedding space (also called **dictionary**) $e \\in \\mathbb{R}^{K \\times D} $ where $K$ is the size of the **discrete** latent space and $D$ is the dimensionality of each latent embedding vector $e_i \\in \\mathbb{R}^{D}$, $i \\in 1,2,\\dots,K$. Denote $N$ the number of latent variables representing the data, $z_e(x) = [z^c_1, z^c_2, \\dots, z^c_N]$ the continous output of the encoder for input $x$ and $z_q(x) = [z_1, z_2, \\dots, z_N]$ its corresponding discretization where $z_e(x)_i=z^c_i \\in \\mathbb{R}^{D}$ and $z_q(x)_i=z_i \\in \\{e_j\\}_{j=1}^{K}$. \\\n",
    "The posterior categorical distribution $q(z|x)$ probabilities are defined as one-hot vectors as follows:\n",
    "$$ q(z_i=e_k|x) = \\begin{cases}\n",
    "  1 & \\mbox{ if $ k=argmin_j\\|z_e(x)_i-e_j\\|_2 $}\\\\\n",
    "  0 & \\mbox{ otherwise}\n",
    "  \\end{cases} $$\n",
    "\n",
    "$q(z_i|x)$ is therefore deterministic and we can write:\n",
    "$$ z_q(x)_i = e_k, \\ \\ k=argmin_j\\|z_e(x)_i-e_j\\|_2 $$\n",
    "\n",
    "This is simply a nearest neighbor assignment of each latent variable $z_i$ in $z_e(x)$ to $e$.\n",
    "\n",
    "Note that the output of the encoder $z_e(x)$ and its discretization $z_q(x)$ are both **sequences** of latent variables, where the discrete variables $\\{z_i\\}_{i=1}^{N}$ all come from the same dictionary. For text or audio, the latent sequence is $1D$ while for images and videos it is a $2D$ and $3D$ grid respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNnRCJnuiykM"
   },
   "source": [
    "<img src=\"assets/vqvae_mnist.png?raw=1\" height=\"300\">\n",
    "\n",
    "[Image source](https://keras.io/examples/generative/vq_vae/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img src=\"assets/vqvae_warcraft.png?raw=1\" height=\"300\">\n",
    "\n",
    "[Image source](https://dl.acm.org/doi/fullHtml/10.1145/3472538.3472584)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn_Y0LhuknKE"
   },
   "source": [
    "The above examples illustrate compact discrete latent representations of simple images, where the discrete values correspond to different colors. We may look at this represntation as a downscaling of the original image where the pixel values no longer necessarily correspond to color but to some other discrete latent attribute (e.g., a combination of shape and color or texture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKOOx4UwlD2S"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/learning\" style=\"height:50px;display:inline\"> Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLJKFGOolmbs"
   },
   "source": [
    "The VQ-VAE loss function consists of 3 terms: reconstruction, vector quantization (VQ) and commitment losses.\n",
    "\n",
    "$$ \\mathcal{L}_{VQ-VAE} = \\mathcal{L}_{rec} + \\mathcal{L}_{VQ} + \\beta\\cdot\\mathcal{L}_{commit} = -log[p(x|z_q(x))] + \\|sg[z_e(x)] - e\\|^2_2 + \\beta\\cdot\\|z_e(x) - sg[e]\\|^2_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvmmw1CppArR"
   },
   "source": [
    "1. *Reconstruction Loss*: $\\mathcal{L}_{rec} = -log[p(x|z_q(x))]$ \\\n",
    "This loss corresponds to the original VAE $ELBO$. Assuming a uniform prior over $z$, the KL term is a constant (exercise: show this), hense we are left with the reconstruction term alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgA5fyVCpIhC"
   },
   "source": [
    "2. VQ Loss: $\\mathcal{L}_{VQ} = \\|sg[z_e(x)] - e\\|^2_2$ \\\n",
    "This loss aims to move the embedding vectors $e$ towards the encoder outputs. This term is only used for updating the dictionary, where $s[\\cdot]$ is the stop-gradient operator which is defined as the identity in the forward computation and has zero partial derivatives, constraining its operand to be a non-updated constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd0Bq2w6pK2O"
   },
   "source": [
    "3. *Commitment Loss*: $\\mathcal{L}_{commit} = \\|z_e(x) - sg[e]\\|^2_2$ \\\n",
    "This loss aims to prevent the encoded representations from moving too often between embeddings in order to stabilize training. Here, the stop-gradient operator is applied to the dictionary values. This loss term is weighted by $\\beta > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOrgCE3hoeTo"
   },
   "source": [
    "<img src=\"https://github.com/lioritan/ee046202-unsupervised-learning-data-analysis/blob/master/assets/vqvae_model.png?raw=1\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDeWEb-JPiLI"
   },
   "source": [
    "#### **Propagating Gradients Through the Dictionary**\n",
    "\n",
    "Updating the encoder parameters with respect to the reconstruction loss requires propagating gradients from the decoder to the encoder. This is problematic since the discretization of the encoder output to the dictionary values is a not a differentiable operation.\n",
    "\n",
    "The solution of the VQ-VAE to this problem is using the **straight-through estimator** for the gradient, which simply copies the gradients from the discretized decoder input to the continous encoder output (see the figure above, left). These gradients contain useful information for how the encoder output has to change to lower the reconstruction loss.\n",
    "\n",
    "To understand why this is true, consider the figure above (right). If the gradients of the discrete $z_q(x)$ (decoder input) signify that it should move right, moving $z_e(x)$ (encoder output) to the right will allow it to be discretized to a different $z_q(x)$ which is closer to that direction in the next forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsdshELIObBb"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/lottery.png\" style=\"height:30px;display:inline\"> Generation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFE9mrHsdhGW"
   },
   "source": [
    "After we have trained a VQ-VAE on our data, we obtain a learned discrete latent representation. We may now use this representation for downstream tasks, one of which is data **generation**.\n",
    "\n",
    "While with the standard VAE we generate new data by decoding samples from our Gaussian prior, doing the same with the VQ-VAE by sampling from a uniform prior is not likely to produce good results (why?).\n",
    "\n",
    "We therefore need to **learn the prior** distribution over our latent data representations that the VQ-VAE has converged to. Once we have done that, we can sample from the learned prior and generate new samples by decoding the latent samples with the VQ-VAE decoder (see outline in the figure below).\n",
    "\n",
    "The VQ-VAE on its own is actually **not a generative model**, and can be more intuitively categorized as a **discrete autoencoder**. To generate new samples, a generative model is trained on the **latent representations** of the data. This makes the generative task easier since modeling a compact representation is more simple than directly modeling high-dimensional noisy data. Once we have a model that is able to generate new latent samples, we convert them to the original high-dimensional form using the VQ-VAE decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img src=\"assets/vqvae_gen.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M93W7tZBQYlV"
   },
   "source": [
    "#### **Learning the Prior with Autoregressive Generative Models**\n",
    "\n",
    "We can essentially use any type of generative model to learn the prior, including Diffusion models, Generative Adversarial Networks (GANs) or even a VAE. Having a discrete latent variable allows using a class of models named **autoregressive** generative models, which we will now describe in slightly more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPEfJgu4Uhxo"
   },
   "source": [
    "The VQ-VAE prior is a categorical distribution over the dictionary entries:\n",
    "\n",
    "$$p(\\mathbf{z}) = p(z_1, z_2, \\dots, z_N), \\ \\ z_i \\in e_1, \\dots, e_K $$\n",
    "\n",
    "We can modify the distribution to be autoregressive using the chain rule of probability:\n",
    "\n",
    "$$p(z_1, z_2, \\dots, z_N) = p(z_1) \\cdot p(z_2, \\dots, z_N| z_1) = p(z_1) \\cdot p(z_2|z_1) \\cdot p(z_3, \\dots, z_N| z_1, z_2) = ‚àè_{i=1}^{N} p(z_i|z_{<i})  $$\n",
    "\n",
    "This view allows us to fit an autoregressive generative model to the latent data by defining an **order** to the latent variables and parameterizing $p(z_i|z_{<i})$ as a neural network such as an RNN, LSTM, or Transformer. In the case of natural language, there is a natural order which corresponds to the order of the words in the text. In the case of images, one can define an arbitrary order to the pixels, e.g., from top left to bottom right in order of rows. In the first VQ-VAE paper, they use the PixelCNN model to do exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn7cUTFweudf"
   },
   "source": [
    "<img src=\"assets/vqvae_transformer.png\" height=\"300\"> &nbsp; &nbsp; &nbsp; &nbsp; <img src=\"assets/vqvae_pixelcnn.png?raw=1\" height=\"300\">\n",
    "\n",
    "Image sources: [Transformer Decoder](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) (left), [PixelCNN](https://arxiv.org/pdf/1606.05328.pdf) (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2f1yeq9etLf"
   },
   "source": [
    "The input to the autoregressive model is all the past latent variables $z_{<i} = (z_1, \\dots, z_{i-1})$ and the output is a **probability distribution** $p(z_i|z_{<i})$ over the dictionary values $e=\\{e_j\\}_{j=1}^{K}$.\n",
    "\n",
    "For more details on autoregressive models and how they are trained see [Deep Learning - ee046211 - Tutorial 7](https://github.com/taldatech/ee046211-deep-learning/blob/main/ee046211_tutorial_07_sequential_tasks_rnn.ipynb).\n",
    "\n",
    "Once we have trained the model, generation is performed by sampling autoregressively from the output categorical distribution $p(z_i|z_{<i})$:\n",
    "1. Sample a $z_1$ (e.g., uniformly)\n",
    "2. For $i=2$ to $N$:\n",
    "  - Feed $z_{<i}$ as input the model to produce $p(z_i|z_{<i})$\n",
    "  - Sample from $p(z_i|z_{<i})$ based on a chosen *strategy* (e.g., uniformly from entries with the top $k$ probabilites)\n",
    "  - Concatente sampled $z_i$ to $z_{<i}$ to produce $z_{<i+1}$\n",
    "\n",
    "**Note**: the neural network output is deterministic, the *sampling strategy* is in charge of producing random samples from the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL2O76TGkP4l"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/paper.png\" style=\"height:30px;display:inline\"> Short Summary\n",
    "---\n",
    "The VQ-VAE's major contribution is its compact discrete latent space which is useful for downstream tasks.\n",
    "\n",
    "One such task is data generation in the latent space which enables faster training and sampling. The process is as follows:\n",
    "\n",
    "1. Learn a discrete latent representation by training the VQ-VAE on the data.\n",
    "2. Learn a prior generative model on the latent representations of the data produced with the VQ-VAE encoder.\n",
    "3. Sample from the learned prior and convert to the original data representation with the VQ-VAE decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wd-c5iZketz"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/stairs.png\" style=\"height:30px;display:inline\"> VQ-VAE 2\n",
    "---\n",
    "The second version of the VQ-VAE introduces two major differences to the model:\n",
    "1. Replacing the VQ Loss term with an exponential moving average (EMA)\n",
    "2. Hierarchical encoding and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHGtpIArrYHN"
   },
   "source": [
    "#### **Updating the Dictionary with an EMA**\n",
    "\n",
    "Instead of learning the dictionary values $e=\\{e_j\\}_{j=1}^{K}$ by minimizing the VQ loss term $\\mathcal{L}_{VQ} = \\|sg[z_e(x)] - e\\|^2_2$, we can simply update them with an EMA based on the batches of encoder outputs. This works well and is more simple as it does not require differentiation.\n",
    "\n",
    "Denote:\n",
    "- $e_i^{(t)}$ the dictionary value $e_i$ after update timestep (batch) $t$\n",
    "- $n_i^{(t)}$ the number of latent variables assigned to $e_i$ at timestep (batch) $t$\n",
    "- $\\{z_{i,j}^{(t)}\\}_{i=1}^{n_i^{(t)}}$ the latent variables assigned to $e_i$ at timestep (batch) $t$\n",
    "\n",
    "Update Rule:\n",
    "$$ e_i^{(t)} = \\frac{m_i^{(t)}}{N_i^{(t)}} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$N_i^{(t)} = \\gamma \\cdot N_i^{(t-1)} + (1-\\gamma) \\cdot n_i^{(t)}$$\n",
    "\n",
    "$$m_i^{(t)} = \\gamma \\cdot m_i^{(t-1)} + (1-\\gamma) \\cdot \\sum_{j=1}^{n_i^{(t)}} z_{i,j}^{(t)}$$\n",
    "\n",
    "\n",
    "$0 <\\gamma < 1$ is typically chosen such that it is very close to $1$, e.g., $\\gamma = 0.99$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4pmm8jC6bQo"
   },
   "source": [
    "<img src=\"assets/vqvae2.png?raw=1\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVto5KjOxlfm"
   },
   "source": [
    "#### **Hierarchical Encoder Architecture**\n",
    "\n",
    "VQ-VAE 2 aims to learn a better representation for more complex and high-resolution images. They propose a *hierarchical structure* with the motivation of modeling local information, such as texture, separately from global information such as shape and geometry of objects. This hierarchy is implemented in the encoder alone. The decoder receives all encoded levels as input.\n",
    "\n",
    "The figure above (left) illustrates the multi-level hierarchical encoder with two levels, each equipped with a **separate latent dictionary**. It is built of multiple encoders, one for each level, and encodes the image in a multi-stage process:\n",
    "1. The top level encodes the image and is then quantized.\n",
    "2. The bottom level encodes the image **conditioned** on the quantized top level and is then quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh-bNAThUqZP"
   },
   "source": [
    "<img src=\"assets/vqvae2_algo.png?raw=1\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAYfvk9xUr-N"
   },
   "source": [
    "Conditioning in this context practically means providing another input to the model. This condition (e.g., the top level quantized encoding) is sometimes treated differently than the main input (e.g., the image). There are many conditioning methods, we will not elaborate on them here.\n",
    "\n",
    "By conditioning each encoded level on the level above it, we **encourage encoding complementary information** about the image in each level.\n",
    "\n",
    "The figure below presents VQ-VAE 2 reconstructions based on different levels of latent encoding. Notice how each latent level adds extra detail to the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFAO8DBy7Kmk"
   },
   "source": [
    "<img src=\"assets/vqvae_hier_example.png?raw=1\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qHBDC2UX9qK"
   },
   "source": [
    "#### **Hierarchical Generation**\n",
    "\n",
    "**Learning the Prior**: Similar to the encoding procedure, the prior is learned in a hierarchical manner. Initially, a generative model is trained on the top level latent representation. In the following stages, separate generative models are trained for each level **conditioned** on the level above it.\n",
    "\n",
    "**Generating New Data**: Sampling from the prior follows the same steps, starting with sampling the top prior and continuing to sample each lower level conditioned on the sampled level above it. The sampled latents from all levels are then provided as input to the decoder to produce a high resolution image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K-ta9WUpvtH"
   },
   "source": [
    "<img src=\"https://img.icons8.com/color/brick.png\" style=\"height:30px;display:inline\"> Applications\n",
    "---\n",
    "\n",
    "The VQ-VAE and similar models are used in many areas of machine learning including Computer Vision (CV), Natural Language Processing (NLP) and Reinforcement Learning (RL).\n",
    "\n",
    "Some of the most well known uses of the VQ-VAE include the first version of [DALL-E](https://openai.com/dall-e-3), a text-conditioned generative model for images, and the model-based RL [Dreamer](https://danijar.com/project/dreamerv3/) algorithm.\n",
    "\n",
    "To see some more interesting applications, check out https://paperswithcode.com/method/vq-vae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img src=\"assets/vqvae_dall-e.png?raw=1\" height=\"300\"> &nbsp; &nbsp; <img src=\"assets/vqvae_dreamer.gif\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwBg5f5WObBi"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Tutorial based on [\"*Neural Discrete Representation Learning*\", Van Den Oord et al. (NeurIPS 2017)](https://arxiv.org/abs/1711.00937) and [\"*Generating Diverse High-Fidelity Images with VQ-VAE-2*\", Razavi, Van Den Oord et al. (NeurIPS 2019)](https://arxiv.org/abs/1906.00446)\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
